{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Formal Simulated Inference***\n",
    "\n",
    "We conduct hypothesis testing on specific feature columns of our data to check whether it forms clusters.\n",
    "\n",
    "**Define Model and Assumptions**\n",
    "\n",
    "We assume that the data assumes a Gaussian mixture model with k components. We want to know what number of clusters/components is the most likely. Therefore, we are going to iteratively perform statistical tests on k clusters vs k+1 clusters, where k ranges from 1 to 4. \n",
    "\n",
    "$F_0\\sim GMM(k)$\n",
    "\n",
    "$F_A\\sim GMM(k+1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Formalize Statistical Test**\n",
    "\n",
    "We formalize it as follows:\n",
    "\n",
    "$H_0: n = k$\n",
    "\n",
    "$H_A: n = k+1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formalize Test Statistic:**\n",
    "\n",
    "$X = -2(\\log(\\lambda))$\n",
    "\n",
    "Here, the $\\lambda$ is the likelihood ratio between the alternative and the null, i.e. \n",
    "\n",
    "$\\lambda = \\frac{likelihood_A^*}{likelihood_0^*}$\n",
    "\n",
    "The * indicates that these likelihoods are optimal. In our implementation we use an EM approach to calculate the optimal $\\lambda$ using the normalmixEM method implemented within the mixtools R package.\n",
    "\n",
    "We first proceed by writing out the necessary code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code ingests the provided dataset, throws out columns 5 and 6, and labels the columns.\n",
    "\n",
    "loadData <- function(filepath = 'C:/Users/iakuznet/Desktop/the-fat-boys/data'){\n",
    "\trequire(data.table)\n",
    "\t\n",
    "\t#The data is composed of 144 columns, with each 6 consecutive columns corresponding to 1 of 24 markers. Apparently column 5 and 6 of the data are unnecessary and should be thrown out.\n",
    "\t#List of fluorescent markers used\n",
    "\tmarkers <- c('Synap_1', 'Synap_2', 'VGlut1_1', 'VGlut1_2', 'VGlut2', 'VGlut3', 'PSD', 'Glur2', 'NDAR1', 'NR2B', 'GAD', 'VGAT', 'PV',\n",
    "\t'Gephyr', 'GABAR1', 'GABABR', 'CR1', '5HT1A', 'NOS', 'TH', 'VACht', 'Synapo', 'Tubuli', 'DAPI')\n",
    "\t\n",
    "\t#What columns 1-4 for each of the florescent markers coorespond to.\n",
    "\tlabels <- c('Int_Bright','Local_Bright','Dist_COM','MOI')\n",
    "\t\n",
    "\t#Generate labels for each column\n",
    "\tcol_labels <- c() \n",
    "\t\n",
    "\tfor (i in 1:length(markers)){\n",
    "\t\tfor (j in 1:length(labels)){\n",
    "\t\t\tcol_labels[(i - 1) * length(labels) + j] = paste(markers[i],\"_\",labels[j],sep = \"\")\n",
    "\t\t}\n",
    "\t}\n",
    "\t\n",
    "\t#Change to relevant location of file\n",
    "\tsetwd(filepath)\n",
    "\n",
    "\t#Load in file using data.tables fread()\n",
    "\tdata <- fread(\"synapsinR_7thA.tif.Pivots.txt.2011Features.txt\")\n",
    "\n",
    "\t#Convert to dataframe\n",
    "\tdata <- as.data.frame(data)\n",
    "\n",
    "\t#Remove columns 5 and 6 corresponding to each fluorescent marker\n",
    "\tgood_ones <- c(1:dim(data)[2])\n",
    "\tdim(good_ones) <- c(6,dim(data)[2] / 6)\n",
    "\tgood_ones <- good_ones[1:4,]\n",
    "\tdim(good_ones) <- c(4 * dim(good_ones)[2],1)\n",
    "\tdata_cleaned <- data[,good_ones]\n",
    "\t\n",
    "\t#Label data\n",
    "\tcolnames(data_cleaned) <- col_labels\n",
    "\t\n",
    "\treturn(data_cleaned)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Subsamples 10000 rows from the original data set for easier analysis\n",
    "\n",
    "subsampleRows <- function(data,samples = 10000,seed = 42){\n",
    "\t#Subsample some number of rows for further analysis. Set seed for reproducibility.\n",
    "\tset.seed(seed);rand_rows = sample(1:dim(data)[1],samples)\n",
    "\tsubsample <- data[rand_rows,]\n",
    "\treturn(subsample)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Wrapper of gmm.test and gmm.power code provided by Jovo. Conducts hypothesis test of H_0 = n gaussians vs H_a = n+1 gaussians for n = 1:maxK and constructs power curves for univariate data.\n",
    "\n",
    "\n",
    "marginal_inference <- function(data,maxK = 3, nboot = 10, power = TRUE){\n",
    "\trequire(data.table)\n",
    "\trequire(ggplot2)\n",
    "\trequire(gtable)\n",
    "\trequire(subspace)\n",
    "\trequire(mixtools)\n",
    "\n",
    "\t#' Testing Routine for GMM \n",
    "\t#' \n",
    "\t#' @param Xdata Univariate data\n",
    "\t#' @param maxK The upper bound on the number of components\n",
    "\t#' @param nboot The number of bootstrap replicates to be used \n",
    "\t#' @return \n",
    "\t#' \n",
    "\t#' \n",
    "\n",
    "\trequire(foreach)\n",
    "\tgmm.test <- function(Xdata,maxK=3,nboot=10) {\n",
    "\t  nsample = length(Xdata)\n",
    "\t  \n",
    "\t  foreach(nK=1:maxK,.errorhandling = 'remove') %do% {\n",
    "\t\tfit.null = go.fit(Xdata,k=nK)\n",
    "\t\tfit.alt  = go.fit(Xdata,k=nK+1)\n",
    "\t\tmy.stat = -2 * (fit.alt$loglik - fit.null$loglik)\n",
    "\t\tboot.dist.null = foreach(itr=1:nboot,.combine='c',.errorhandling = 'remove') %do% {\n",
    "\t\t  Xboot = go.sim(nsample = nsample, param = fit.null,k = nK)\n",
    "\t\t  fit.null.boot = go.fit(Xboot,k=nK)\n",
    "\t\t  fit.alt.boot = go.fit(Xboot,k=nK+1)\n",
    "\t\t  -2 * (fit.alt.boot$loglik - fit.null.boot$loglik)\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\tmy.ecdf = ecdf(boot.dist.null)\n",
    "\t\tlist(nK=nK, fitted=fit.null, stat=my.stat, pval= my.ecdf(my.stat))\n",
    "\t  }\n",
    "\t}\n",
    "\n",
    "\t#' Power calculation routine for GMM \n",
    "\t#' \n",
    "\t#' @param nullparam \n",
    "\t#' @param altparam\n",
    "\t#' @param alpha\n",
    "\t#' @param maxmc\n",
    "\t#' @return\n",
    "\t#' \n",
    "\tgmm.power <- function(nsample,nullK, altK, nullparam, altparam, alpha=0.05, maxmc = 10) {\n",
    "\t  boot.dist.null = foreach(itr=1:maxmc,.combine='c',.errorhandling = 'remove') %do% {\n",
    "\t\tXboot = go.sim(nsample,nullparam,nullK)\n",
    "\t\tfit.null.boot = go.fit(Xboot,k = nullK)\n",
    "\t\tfit.alt.boot = go.fit(Xboot,k=altK)\n",
    "\t\t-2 * (fit.alt.boot$loglik - fit.null.boot$loglik)\n",
    "\t  }\n",
    "\t  \n",
    "\t  boot.dist.alt = foreach(itr=1:maxmc,.combine='c',.errorhandling = 'remove') %do% {\n",
    "\t\tXboot = go.sim(nsample,altparam, altK)\n",
    "\t\tfit.null.boot = go.fit(Xboot,k = nullK)\n",
    "\t\tfit.alt.boot = go.fit(Xboot,k=altK)\n",
    "\t\t-2 * (fit.alt.boot$loglik - fit.null.boot$loglik)\n",
    "\t  }\n",
    "\t  \n",
    "\t  cval = quantile(boot.dist.null,alpha)\n",
    "\t  power_alt <- ecdf(boot.dist.alt)(cval)\n",
    "\t  power_null <- ecdf(boot.dist.null)(cval)\n",
    "\t  c(power_alt,power_null)\n",
    "\t  \n",
    "\t}\n",
    "\n",
    "\tgo.fit <- function(Xdata,k) {\n",
    "\t  if(k> 1) {\n",
    "\t\tnormalmixEM(Xdata,k=k)\n",
    "\t  } else {\n",
    "\t\tlist(mean=mean(Xdata),sd=sd(Xdata),loglik=sum(log(dnorm(Xdata,mean=mean(Xdata),sd=sd(Xdata)))))\n",
    "\t  }\n",
    "\t}\n",
    "\n",
    "\tgo.sim <- function(nsample,param,k) {\n",
    "\t  if(k > 1) {\n",
    "\t\trnormmix(nsample, lambda=param$lambda, mu=param$mu, sigma =param$sigma)\n",
    "\t  } else {\n",
    "\t\trnorm(nsample,mean=param$mean,sd=param$sd)\n",
    "\t  }\n",
    "\t}\n",
    "\t\n",
    "\ttest_result <- gmm.test(data,maxK,nboot)\n",
    "\tpower_curve <-c()\n",
    "\tif (power){\n",
    "\t\tdomain <- 20 * c(1:25)\n",
    "\t\ttemp <- cbind(rep(0,25),rep(0,25))\n",
    "\t\tpower_curve <- domain\n",
    "\t\tfor (i in c(1:(maxK - 1))){\n",
    "\t\t\tfor (j in c(1:25)){\n",
    "\t\t\t\ttemp[j,] <- gmm.power(nsample = domain[j],nullK = i, altK = i + 1,nullparam = test_result[[i]]$fitted, altparam = test_result[[i + 1]]$fitted,maxmc = nboot)\n",
    "\t\t\t}\n",
    "\t\t\tpower_curve <- cbind(power_curve,temp)\n",
    "\t\t\tcolnames(power_curve)[2 * i] <- paste(i,\"versus\",i + 1,\"H_a\",sep=\"\")\n",
    "\t\t\tcolnames(power_curve)[2 * i + 1] <- paste(i,\"versus\",i + 1,\"H_0\",sep=\"\")\n",
    "\t\t}\n",
    "\t\tcolnames(power_curve)[1] <- \"Samples\"\n",
    "\t\tpower_curve <- as.data.frame(power_curve)\n",
    "\t}\n",
    "\tresult <- list(test_result,power_curve)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Driver funciton for marginal_inference. Feeds individual columns from input data to marginal_inference to conduct hypothesis tests.\n",
    "\n",
    "conductInference <- function(data,maxK = 3, nboot = 10, power = TRUE, columns_to_analyze = 4 * c(0:23) + 1){\n",
    "\trequire(ggplot2)\n",
    "\trequire(reshape)\n",
    "\t\n",
    "\tplots <- list()\n",
    "\ttests_to_run <- c()\n",
    "\tfor (i in 1:maxK - 1){\n",
    "\t\ttests_to_run[i] <- paste(\"k =\",i,\"versus k =\",i + 1)\n",
    "\t}\n",
    "\t\n",
    "\tfor (i in columns_to_analyze){\n",
    "\t\tcapture.output(test_result <- marginal_inference(data[,i],maxK = maxK, nboot = nboot, power = power),file = \"NUL\")\n",
    "\t\tpvals <- rep(-1,maxK-1)\n",
    "\t\ttest <- test_result[[1]]\n",
    "\t\tpower_curve <- test_result[[2]]\n",
    "\t\tp1 <- list()\n",
    "\t\tcolnames(power_curve)[2:length(colnames(power_curve))] <- paste(\"c\",c(2:length(power_curve)),sep=\"\")\n",
    "\t\tfor (j in c(1:(maxK - 1))){\n",
    "\t\t\tdf <- power_curve[,c(1,2 * j, 2 * j + 1)]\n",
    "\t\t\tdf <- melt(df,\"Samples\")\n",
    "\t\t\tp1[[j]] <- ggplot(df,aes_string(x = \"Samples\", y = \"value\", color = \"variable\")) + geom_point() + xlab(\"Samples\") + ylab(\"Power\") + ggtitle(paste(\"Power for test of k =\",j,\"versus k =\",j + 1)) + scale_colour_manual(\"Distribution\",values = c(\"blue\",\"red\"),labels=c(\"Alt\", \"Null\"))\n",
    "\t\t\tprint(p1[[j]])\n",
    "\t\t\tpvals[j] <- test[[j]]$pval\t\t\n",
    "\t\t}\t\n",
    "\t\tpval_df <- as.data.frame(cbind(tests_to_run,pvals))\n",
    "\t\tp2 <- ggplot(pval_df,aes(x = tests_to_run, y = pvals)) + geom_point() + xlab(\"Test\") + ylab(\"P-value\") + ggtitle(\"Inference Testing Results\")\n",
    "\t\tprint(p2)\n",
    "\t\tplots <- list(plots, p1,p2)\n",
    "\t\t\n",
    "\t}\n",
    "\t\n",
    "\treturn(plots)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in the required data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: data.table\n",
      "Warning message:\n",
      ": package 'data.table' was built under R version 3.1.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Read 0.0% of 1119299 rows\r",
      "Read 0.9% of 1119299 rows\r",
      "Read 1.8% of 1119299 rows\r",
      "Read 2.7% of 1119299 rows\r",
      "Read 3.6% of 1119299 rows\r",
      "Read 4.5% of 1119299 rows\r",
      "Read 5.4% of 1119299 rows\r",
      "Read 6.3% of 1119299 rows\r",
      "Read 7.1% of 1119299 rows\r",
      "Read 8.0% of 1119299 rows\r",
      "Read 8.9% of 1119299 rows\r",
      "Read 9.8% of 1119299 rows\r",
      "Read 10.7% of 1119299 rows\r",
      "Read 11.6% of 1119299 rows\r",
      "Read 12.5% of 1119299 rows\r",
      "Read 13.4% of 1119299 rows\r",
      "Read 14.3% of 1119299 rows\r",
      "Read 15.2% of 1119299 rows\r",
      "Read 16.1% of 1119299 rows\r",
      "Read 17.0% of 1119299 rows\r",
      "Read 17.9% of 1119299 rows\r",
      "Read 18.8% of 1119299 rows\r",
      "Read 19.7% of 1119299 rows\r",
      "Read 20.5% of 1119299 rows\r",
      "Read 21.4% of 1119299 rows\r",
      "Read 22.3% of 1119299 rows\r",
      "Read 23.2% of 1119299 rows\r",
      "Read 24.1% of 1119299 rows\r",
      "Read 25.0% of 1119299 rows\r",
      "Read 25.9% of 1119299 rows\r",
      "Read 26.8% of 1119299 rows\r",
      "Read 27.7% of 1119299 rows\r",
      "Read 28.6% of 1119299 rows\r",
      "Read 29.5% of 1119299 rows\r",
      "Read 30.4% of 1119299 rows\r",
      "Read 31.3% of 1119299 rows\r",
      "Read 32.2% of 1119299 rows\r",
      "Read 33.1% of 1119299 rows\r",
      "Read 33.9% of 1119299 rows\r",
      "Read 34.8% of 1119299 rows\r",
      "Read 35.7% of 1119299 rows\r",
      "Read 36.6% of 1119299 rows\r",
      "Read 37.5% of 1119299 rows\r",
      "Read 38.4% of 1119299 rows\r",
      "Read 39.3% of 1119299 rows\r",
      "Read 40.2% of 1119299 rows\r",
      "Read 41.1% of 1119299 rows\r",
      "Read 42.0% of 1119299 rows\r",
      "Read 42.9% of 1119299 rows\r",
      "Read 43.8% of 1119299 rows\r",
      "Read 44.7% of 1119299 rows\r",
      "Read 45.6% of 1119299 rows\r",
      "Read 46.5% of 1119299 rows\r",
      "Read 47.4% of 1119299 rows\r",
      "Read 48.2% of 1119299 rows\r",
      "Read 49.1% of 1119299 rows\r",
      "Read 50.0% of 1119299 rows\r",
      "Read 50.9% of 1119299 rows\r",
      "Read 51.8% of 1119299 rows\r",
      "Read 52.7% of 1119299 rows\r",
      "Read 53.6% of 1119299 rows\r",
      "Read 54.5% of 1119299 rows\r",
      "Read 55.4% of 1119299 rows\r",
      "Read 56.3% of 1119299 rows\r",
      "Read 57.2% of 1119299 rows\r",
      "Read 58.1% of 1119299 rows\r",
      "Read 59.0% of 1119299 rows\r",
      "Read 59.9% of 1119299 rows\r",
      "Read 60.8% of 1119299 rows\r",
      "Read 61.6% of 1119299 rows\r",
      "Read 62.5% of 1119299 rows\r",
      "Read 63.4% of 1119299 rows\r",
      "Read 64.3% of 1119299 rows\r",
      "Read 65.2% of 1119299 rows\r",
      "Read 66.1% of 1119299 rows\r",
      "Read 67.0% of 1119299 rows\r",
      "Read 67.9% of 1119299 rows\r",
      "Read 68.8% of 1119299 rows\r",
      "Read 69.7% of 1119299 rows\r",
      "Read 70.6% of 1119299 rows\r",
      "Read 71.5% of 1119299 rows\r",
      "Read 72.4% of 1119299 rows\r",
      "Read 73.3% of 1119299 rows\r",
      "Read 74.2% of 1119299 rows\r",
      "Read 75.0% of 1119299 rows\r",
      "Read 75.9% of 1119299 rows\r",
      "Read 76.8% of 1119299 rows\r",
      "Read 77.7% of 1119299 rows\r",
      "Read 78.6% of 1119299 rows\r",
      "Read 79.5% of 1119299 rows\r",
      "Read 80.4% of 1119299 rows\r",
      "Read 81.3% of 1119299 rows\r",
      "Read 82.2% of 1119299 rows\r",
      "Read 83.1% of 1119299 rows\r",
      "Read 84.0% of 1119299 rows\r",
      "Read 84.9% of 1119299 rows\r",
      "Read 85.8% of 1119299 rows\r",
      "Read 86.7% of 1119299 rows\r",
      "Read 87.6% of 1119299 rows\r",
      "Read 88.4% of 1119299 rows\r",
      "Read 89.3% of 1119299 rows\r",
      "Read 90.2% of 1119299 rows\r",
      "Read 91.1% of 1119299 rows\r",
      "Read 92.0% of 1119299 rows\r",
      "Read 92.9% of 1119299 rows\r",
      "Read 93.8% of 1119299 rows\r",
      "Read 94.7% of 1119299 rows\r",
      "Read 95.6% of 1119299 rows\r",
      "Read 96.5% of 1119299 rows\r",
      "Read 97.4% of 1119299 rows\r",
      "Read 98.3% of 1119299 rows\r",
      "Read 99.2% of 1119299 rows\r",
      "Read 1119299 rows and 144 (of 144) columns from 1.817 GB file in 00:04:30\n"
     ]
    }
   ],
   "source": [
    "data <- loadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then subsample 300 samples. Note that this is sufficient, as we will test up to k = 6, which means that the largest gaussian mixture model that we will need to fit (recall that we are testing on univariate data) will have 6+6+(6-1) = 23 degree of freedom. Hence, we have over 10 data points for each degree of freedom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subsample <- subsampleRows(data,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to conduct our hypothesis testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "Loading required package: reshape\n",
      "Loading required package: gtable\n",
      "Warning message:\n",
      ": package 'gtable' was built under R version 3.1.3Loading required package: grid\n",
      "Loading required package: subspace\n",
      "Warning message:\n",
      ": package 'subspace' was built under R version 3.1.3Loading required package: mixtools\n",
      "Warning message:\n",
      ": package 'mixtools' was built under R version 3.1.3Loading required package: boot\n",
      "Loading required package: MASS\n",
      "Loading required package: segmented\n",
      "Warning message:\n",
      ": package 'segmented' was built under R version 3.1.3mixtools package, version 1.0.3, Released 2015-04-18\n",
      "This package is based upon work supported by the National Science Foundation under Grant No. SES-0518772.\n",
      "\n",
      "\n",
      "Attaching package: 'mixtools'\n",
      "\n",
      "The following object is masked from 'package:grid':\n",
      "\n",
      "    depth\n",
      "\n",
      "Loading required package: foreach\n",
      "Warning message:\n",
      ": package 'foreach' was built under R version 3.1.3"
     ]
    }
   ],
   "source": [
    "r1 <- conductInference(subsample,maxK = 6, nboot = 20, power = TRUE, columns_to_analyze = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
